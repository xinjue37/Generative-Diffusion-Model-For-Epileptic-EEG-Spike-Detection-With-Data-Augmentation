{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1923d7aed50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt \n",
    "from PIL import Image\n",
    "from utility import *\n",
    "from model import *\n",
    "\n",
    "MODEL_FILE_DIRC_WaveGrad = MODEL_FILE_DIRC + \"/WaveGrad\"\n",
    "os.makedirs(MODEL_FILE_DIRC_WaveGrad, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "* Get the train, validation, test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data from EEG_csv/eeg1.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (3840, 19)\n",
      "Data with   spike: (3840, 19)\n",
      "Data after  split into window: (3, 1280, 19)\n",
      "Labels: (3,)\n",
      "Num spike: 3\n",
      "EEG1 has 3 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg2.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(160, 1280, 19)\n",
      "EEG2 has 160 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg3.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(153, 1280, 19)\n",
      "EEG3 has 153 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg4.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(170, 1280, 19)\n",
      "EEG4 has 170 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg5.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(148, 1280, 19)\n",
      "EEG5 has 148 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg6.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(207, 1280, 19)\n",
      "EEG6 has 207 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg7.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(147, 1280, 19)\n",
      "EEG7 has 147 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg8.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (3840, 19)\n",
      "Data with   spike: (3840, 19)\n",
      "Data after  split into window: (3, 1280, 19)\n",
      "Labels: (3,)\n",
      "Num spike: 3\n",
      "EEG8 has 3 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg9.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (3840, 19)\n",
      "Data with   spike: (3840, 19)\n",
      "Data after  split into window: (3, 1280, 19)\n",
      "Labels: (3,)\n",
      "Num spike: 3\n",
      "EEG9 has 3 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg10.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(109, 1280, 19)\n",
      "EEG10 has 109 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg11.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(251, 1280, 19)\n",
      "EEG11 has 251 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg12.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (34560, 19)\n",
      "Data with   spike: (34560, 19)\n",
      "Data after  split into window: (27, 1280, 19)\n",
      "Labels: (27,)\n",
      "Num spike: 27\n",
      "EEG12 has 27 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg13.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(1101, 1280, 19)\n",
      "EEG13 has 1101 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg14.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(190, 1280, 19)\n",
      "EEG14 has 190 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg15.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (215040, 19)\n",
      "Data with   spike: (215040, 19)\n",
      "Data after  split into window: (168, 1280, 19)\n",
      "Labels: (168,)\n",
      "Num spike: 168\n",
      "EEG15 has 168 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg16.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(89, 1280, 19)\n",
      "EEG16 has 89 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg17.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (203520, 19)\n",
      "Data with   spike: (203520, 19)\n",
      "Data after  split into window: (159, 1280, 19)\n",
      "Labels: (159,)\n",
      "Num spike: 159\n",
      "EEG17 has 159 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg18.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (533760, 19)\n",
      "Data with   spike: (533760, 19)\n",
      "Data after  split into window: (417, 1280, 19)\n",
      "Labels: (417,)\n",
      "Num spike: 417\n",
      "EEG18 has 417 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg19.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (157440, 19)\n",
      "Data with   spike: (157440, 19)\n",
      "Data after  split into window: (123, 1280, 19)\n",
      "Labels: (123,)\n",
      "Num spike: 123\n",
      "EEG19 has 123 windows of data \n",
      "\n",
      "\n",
      "> > > Train    data  has shape: torch.Size([2542, 19, 1280]) when duration = 10 seconds\n",
      "> > > Data after DWT has shape: torch.Size([2542, 19, 1282])\n",
      "> > > Label              shape: torch.Size([2542])\n",
      "CPU times: total: 1min 55s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloaders, num_data = get_dataloader(list(range(1,20)), shuffle=True)\n",
    "    \n",
    "num_train_data, num_valid_data, num_test_data = num_data\n",
    "train_data, valid_data, test_data             = dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model, EMA model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-class of WaveGrad\n",
    "|No|Class |Structure|\n",
    "|:-:|:-:|:--|\n",
    "|1|Conv1dWithInitialization|Implment Convolutional 1D layer with heuristic initialization|\n",
    "|2|ConvolutionBlock|leaky_relu --> Conv1dWithInitialization |\n",
    "|3|BasicModulationBlock|affin transform (ax+b) --> leaky_relu --> Conv1dWithInitialization |\n",
    "|4|FeatureWiseAffine|[scale * x + shift]|\n",
    "|5|FeatureWiseLinearModulation|(Conv1dWithInitialization --> leaky_relu + PositionalEmbedding) --> 2 Conv1dWithInitialization produce 2 output|\n",
    "|6|PositionalEmbedding|Return the position encoding value based on the noise level|\n",
    "|7|InterpolationBlock|Downsample OR Upsample a datasize by nn.functional.interpolation|\n",
    "|8|UpsamplingBlock|Residual block to perform up sampling of data|\n",
    "|9|DownsamplingBlock|Residual block to perform down sampling of data|\n",
    "|10|EMA|Stabilizing training with Exponential Moving Average (EMA)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been loaded from the file 'Advanced_Diffusion_80.pt'\n",
      "The dataframe that record the loss have been loaded from {MODEL_FILE_DIRC_WaveGrad}/Loss.csv\n"
     ]
    }
   ],
   "source": [
    "model     = WaveGradNN(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "ema       = EMA(0.9) \n",
    "ema.register(model)\n",
    "\n",
    "\n",
    "list_model = os.listdir(MODEL_FILE_DIRC_WaveGrad)\n",
    "if len(list_model) > 0:    # Load the latest trained model\n",
    "    if os.path.exists(f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_best.pt\"):\n",
    "        state_dict_loaded    = torch.load(f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_best.pt\")\n",
    "        prev_best_valid_loss = state_dict_loaded[\"valid_loss\"]\n",
    "    list_model.remove(\"Advanced_Diffusion_best.pt\")\n",
    "    num_list   = [int(model_dir[model_dir.rindex(\"_\") +1: model_dir.rindex(\".\")]) for model_dir in list_model if model_dir.endswith(\".pt\")]\n",
    "    num_max    = np.max(num_list)\n",
    "    \n",
    "    state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_{num_max}.pt\")\n",
    "    model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "    ema.load_state_dict(state_dict_loaded[\"model\"])\n",
    "    EPOCH_START = state_dict_loaded[\"epoch\"] + 1\n",
    "    \n",
    "    print(f\"The model has been loaded from the file 'Advanced_Diffusion_{num_max}.pt'\")\n",
    "\n",
    "    if os.path.exists(f\"{MODEL_FILE_DIRC_WaveGrad}/Loss.csv\"):\n",
    "        df = pd.read_csv(f\"{MODEL_FILE_DIRC_WaveGrad}/Loss.csv\")\n",
    "        df = df.iloc[:EPOCH_START-1, :]\n",
    "        print(f\"The dataframe that record the loss have been loaded from {{MODEL_FILE_DIRC_WaveGrad}}/Loss.csv\")\n",
    "\n",
    "else:\n",
    "    EPOCH_START          = 1\n",
    "    prev_best_valid_loss = 10000 \n",
    "    df                   = pd.DataFrame(columns = [\"Train Loss\", \"Valid Loss\"])\n",
    "\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model infomation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Device used    : cuda\n",
      "BATCH SIZE     : 32\n",
      "MAX_COUNT      : 30\n",
      "LEARNING RATE  : 0.001\n",
      "Previous best validation loss  : 0.005866221658213143\n",
      "Number of EPOCH for training   : 10000 (EPOCH start from 81)\n",
      "Num of epochs of data for train: 2542\n",
      "Num of epochs of data for valid: 543\n",
      "Model parameters               : 34,342,899\n"
     ]
    }
   ],
   "source": [
    "seperate = \"\\n\" + \"-\" * 100 + \"\\n\"\n",
    "print(seperate + \"Model infomation\" + seperate)\n",
    "print(f\"Device used    :\", device)\n",
    "print(f\"BATCH SIZE     :\", BATCH_SIZE)\n",
    "print(f\"MAX_COUNT      :\", MAX_COUNT)\n",
    "print(f\"LEARNING RATE  :\", LEARNING_RATE)\n",
    "print(f\"Previous best validation loss  :\", prev_best_valid_loss)\n",
    "print(f\"Number of EPOCH for training   :\",NUM_EPOCHS, f\"(EPOCH start from {EPOCH_START})\")\n",
    "print(f\"Num of epochs of data for train:\", num_train_data)\n",
    "print(f\"Num of epochs of data for valid:\", num_valid_data)\n",
    "print(f'Model parameters               : {sum(p.numel() for p in model.parameters()):,}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sample = 3628\n",
    "# num_spike    = 900\n",
    "# ((total_sample-num_spike)*2)/(num_spike*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(string_to_print):\n",
    "    with open(f'{MODEL_FILE_DIRC_WaveGrad}/log.txt', \"a\") as f:\n",
    "        print(string_to_print, file=f)\n",
    "    print(string_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     : 81\n",
      "Train loss: 0.007472192362994307\n",
      "Valid loss: 0.006279281489756885\n",
      "Epoch     : 82\n",
      "Train loss: 0.006359019727804453\n",
      "Valid loss: 0.00624229153875488\n",
      "Epoch     : 83\n",
      "Train loss: 0.006213729548979518\n",
      "Valid loss: 0.006263506195712046\n",
      "Epoch     : 84\n",
      "Train loss: 0.006327592510580547\n",
      "Valid loss: 0.006342028276257313\n",
      "Epoch     : 85\n",
      "Train loss: 0.006402581549131617\n",
      "Valid loss: 0.006121369987781096\n",
      "Epoch     : 86\n",
      "Train loss: 0.0064045293102763376\n",
      "Valid loss: 0.005983684465810519\n",
      "Epoch     : 87\n",
      "Train loss: 0.006230748234350413\n",
      "Valid loss: 0.006178213719065896\n",
      "Epoch     : 88\n",
      "Train loss: 0.006237010356481389\n",
      "Valid loss: 0.0060212442284350454\n",
      "Epoch     : 89\n",
      "Train loss: 0.006295418910488756\n",
      "Valid loss: 0.006249430567081023\n",
      "Epoch     : 90\n",
      "Train loss: 0.006205100229084726\n",
      "Valid loss: 0.006444579357380805\n",
      "Epoch     : 91\n",
      "Train loss: 0.006443939305824336\n",
      "Valid loss: 0.005811724214922658\n",
      "Epoch     : 92\n",
      "Train loss: 0.006230895727708713\n",
      "Valid loss: 0.006582937846526257\n",
      "Epoch     : 93\n",
      "Train loss: 0.006368237145297833\n",
      "Valid loss: 0.006656634708794441\n",
      "Epoch     : 94\n",
      "Train loss: 0.0063478611497593904\n",
      "Valid loss: 0.0065132751100410195\n",
      "Epoch     : 95\n",
      "Train loss: 0.006225840781355917\n",
      "Valid loss: 0.005961720648171932\n",
      "Epoch     : 96\n",
      "Train loss: 0.0061252476789649315\n",
      "Valid loss: 0.006562031462486717\n",
      "Epoch     : 97\n",
      "Train loss: 0.006178358653274509\n",
      "Valid loss: 0.006715064220006953\n",
      "Epoch     : 98\n",
      "Train loss: 0.006427710663371907\n",
      "Valid loss: 0.00623515640723332\n",
      "Epoch     : 99\n",
      "Train loss: 0.006106560028368427\n",
      "Valid loss: 0.0061498938015152734\n",
      "Epoch     : 100\n",
      "Train loss: 0.006324133213629974\n",
      "Valid loss: 0.005951223068254868\n",
      "Epoch     : 101\n",
      "Train loss: 0.006075348397299214\n",
      "Valid loss: 0.006460919839023007\n",
      "Epoch     : 102\n",
      "Train loss: 0.006222784736088933\n",
      "Valid loss: 0.005982396626779827\n",
      "Epoch     : 103\n",
      "Train loss: 0.00609004923694252\n",
      "Valid loss: 0.0062107090352868065\n",
      "Epoch     : 104\n",
      "Train loss: 0.006090779956591401\n",
      "Valid loss: 0.006342751490257502\n",
      "Epoch     : 105\n",
      "Train loss: 0.006303746449628465\n",
      "Valid loss: 0.006161113497741095\n",
      "Epoch     : 106\n",
      "Train loss: 0.006100072454365101\n",
      "Valid loss: 0.006357322712849078\n",
      "Epoch     : 107\n",
      "Train loss: 0.006063566028040099\n",
      "Valid loss: 0.006497913494733596\n",
      "Epoch     : 108\n",
      "Train loss: 0.006071637140588438\n",
      "Valid loss: 0.006320762974561688\n",
      "Epoch     : 109\n",
      "Train loss: 0.006282244220575322\n",
      "Valid loss: 0.0060978023880753065\n",
      "Epoch     : 110\n",
      "Train loss: 0.006339237211165139\n",
      "Valid loss: 0.00618439896330649\n",
      "Epoch     : 111\n",
      "Train loss: 0.006274284046058295\n",
      "Valid loss: 0.005976981660059583\n",
      "Epoch     : 112\n",
      "Train loss: 0.006083560138823572\n",
      "Valid loss: 0.00637176937490537\n",
      "Epoch     : 113\n",
      "Train loss: 0.006044703602415665\n",
      "Valid loss: 0.006286562422142942\n",
      "Epoch     : 114\n",
      "Train loss: 0.006191006412452642\n",
      "Valid loss: 0.006137588872654882\n",
      "Epoch     : 115\n",
      "Train loss: 0.006052234492181887\n",
      "Valid loss: 0.006232559132093004\n",
      "Epoch     : 116\n",
      "Train loss: 0.006039979266051136\n",
      "Valid loss: 0.006054365064118668\n",
      "Epoch     : 117\n",
      "Train loss: 0.006208233277362505\n",
      "Valid loss: 0.006094981540632511\n",
      "Epoch     : 118\n",
      "Train loss: 0.00593696403911412\n",
      "Valid loss: 0.006136805175856771\n",
      "Epoch     : 119\n",
      "Train loss: 0.006238249321794247\n",
      "Valid loss: 0.005931183009736007\n",
      "Epoch     : 120\n",
      "Train loss: 0.006109899365695605\n",
      "Valid loss: 0.00636211301081747\n",
      "Epoch     : 121\n",
      "Train loss: 0.0061009630713286505\n",
      "Valid loss: 0.00600813493873533\n",
      "The validation loss is continuous decrease for 30 time, so training stop\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH_START, NUM_EPOCHS):\n",
    "    \n",
    "        ## 1. Training\n",
    "        model.train()\n",
    "        train_loss = train(model, ema, train_data, device, num_train_data, optimizer) \n",
    "        \n",
    "        ## 2. Evaluating\n",
    "        model.eval()\n",
    "        valid_loss = evaluate(model, valid_data, device, num_valid_data) \n",
    "            \n",
    "        ## 3. Show the result\n",
    "        df.loc[len(df)] = [train_loss, valid_loss]\n",
    "        print_log(f\"Epoch     : {epoch}\")\n",
    "        print_log(f\"Train loss: {train_loss}\")\n",
    "        print_log(f\"Valid loss: {valid_loss}\")\n",
    "        \n",
    "        # After 30 iteration, Random Get  1 epoch of data from validation loader, \n",
    "        # and visualize the reconstructed output\n",
    "        if epoch >= 30 and epoch % 10 == 0:\n",
    "            plt.ioff()\n",
    "            model = model.to(\"cpu\")\n",
    "            diffuse_num_step     = 100\n",
    "            reconstruct_num_step = 100\n",
    "\n",
    "            iterator = iter(valid_data)\n",
    "            original_signal, signal_DWT, label = next(iterator)\n",
    "            rand_num = np.random.randint(0, original_signal.shape[0])\n",
    "            x_0 = original_signal[rand_num][None,:]\n",
    "            x_0_DWT = signal_DWT[rand_num][None,:]\n",
    "            label        = label[rand_num].item()\n",
    "            eps          = torch.randn_like(x_0)\n",
    "            initial_x    = q_sample(x_0, alphas_prod_p_sqrt[diffuse_num_step-1], eps)\n",
    "\n",
    "            x_seq   = p_sample_loop(model, \n",
    "                                    x_0.shape, \n",
    "                                    label   = torch.ones((1), dtype=torch.int64) if label==1 else torch.zeros((1), dtype=torch.int64),\n",
    "                                    n_steps = reconstruct_num_step, \n",
    "                                    initial_x = initial_x,\n",
    "                                    x_DWT = x_0_DWT, \n",
    "                                    device=torch.device(\"cpu\"))  \n",
    "\n",
    "            # Reverse the label\n",
    "            x_seq1   = p_sample_loop(model, \n",
    "                                    x_0.shape, \n",
    "                                    label   = torch.zeros((1), dtype=torch.int64) if label==1 else torch.ones((1), dtype=torch.int64),\n",
    "                                    n_steps = reconstruct_num_step, \n",
    "                                    initial_x = initial_x,\n",
    "                                    x_DWT = x_0_DWT, \n",
    "                                    device=torch.device(\"cpu\"))\n",
    "            fig, axs = plt.subplots(len(AVE_CHANNELS_NAME), 1, figsize=(20, 50))\n",
    "\n",
    "            sample = \"Annotated\" if label == 1 else \"Un-annotated\"\n",
    "            seperate = 1\n",
    "            for i, col in enumerate(AVE_CHANNELS_NAME):\n",
    "                axs[i].set_title(col + sample)\n",
    "                axs[i].plot(x_0[0][i], label=f\"Original Signal\")\n",
    "                axs[i].plot(x_seq[0][0, i] + seperate, label=f\"Noisy Signal_{diffuse_num_step}\")\n",
    "                axs[i].plot(x_seq[-1][0, i] - seperate, label=f\"Reconstructed Signal_{reconstruct_num_step}\")\n",
    "                axs[i].plot(x_seq1[-1][0, i] - seperate*2,label=f\"Reconstructed Signal_{reconstruct_num_step}_if_label_opposite\")\n",
    "                axs[i].legend()\n",
    "\n",
    "            SAVE_PATH = f'{MODEL_FILE_DIRC_WaveGrad}/Epoch_{epoch}_Signal.png'\n",
    "            plt.savefig(SAVE_PATH, transparent=False, facecolor='white')\n",
    "\n",
    "            fig, axs = plt.subplots(len(AVE_CHANNELS_NAME), 1, figsize=(20, 50))\n",
    "            seperate = 1e-4\n",
    "            for i, col in enumerate(AVE_CHANNELS_NAME):\n",
    "                axs[i].set_title(col + sample)\n",
    "                axs[i].plot(inverse_mu_law(x_0[0][i] * NORMALIZE_CONS_2) * NORMALIZE_CONS_1, label=f\"Original Signal\")\n",
    "                axs[i].plot(inverse_mu_law(x_seq[-1][0, i] * NORMALIZE_CONS_2) * NORMALIZE_CONS_1 - seperate, label=f\"Reconstructed Signal_{reconstruct_num_step}\")\n",
    "                axs[i].plot(inverse_mu_law(x_seq1[-1][0, i] * NORMALIZE_CONS_2) * NORMALIZE_CONS_1 - seperate*2,\n",
    "                            label=f\"Reconstructed Signal_{reconstruct_num_step}_if_label_opposite\")\n",
    "                axs[i].legend()\n",
    "\n",
    "            SAVE_PATH1 = f'{MODEL_FILE_DIRC_WaveGrad}/Epoch_{epoch}_Original_Signal.png'\n",
    "            plt.savefig(SAVE_PATH1, transparent=False, facecolor='white')\n",
    "\n",
    "            model = model.to(\"cuda\")\n",
    "            plt.close('all')\n",
    "        \n",
    "        ## 4.4 Plot the loss function\n",
    "        plt.plot(range(len(df[\"Train Loss\"])), df[\"Train Loss\"], label=\"Train Loss\")\n",
    "        plt.plot(range(len(df[\"Train Loss\"])), df[\"Valid Loss\"], label=\"Valid Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{MODEL_FILE_DIRC_WaveGrad}/Loss.png', transparent=False, facecolor='white')\n",
    "        plt.close('all')\n",
    "\n",
    "        ## 4.5. Save model and Stoping criteria\n",
    "        if prev_best_valid_loss > valid_loss:  # If previous validation loss larger than current validation loss (The model is performed better)\n",
    "            state_dict = {\n",
    "                \"model\": model.state_dict(), \n",
    "                \"epoch\":epoch,\n",
    "                \"valid_loss\": valid_loss\n",
    "            }\n",
    "            torch.save(state_dict, f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_best.pt\")\n",
    "            \n",
    "            prev_best_valid_loss = valid_loss  # Previous validation loss = Current validation loss\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            state_dict = {\n",
    "                \"model\": model.state_dict(), \n",
    "                \"epoch\":epoch,\n",
    "                \"valid_loss\": valid_loss\n",
    "            }\n",
    "            torch.save(state_dict, f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_{epoch}.pt\")\n",
    "        \n",
    "        df.to_csv(f\"{MODEL_FILE_DIRC_WaveGrad}/Loss.csv\", index=False)\n",
    "        \n",
    "        if count == MAX_COUNT:\n",
    "            print_log(f\"The validation loss is continuous decrease for {MAX_COUNT} time, so training stop\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
