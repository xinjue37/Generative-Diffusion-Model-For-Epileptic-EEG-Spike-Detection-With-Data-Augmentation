{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24744b59d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt \n",
    "from PIL import Image\n",
    "from utility import *\n",
    "from model import *\n",
    "from torchsummary import summary\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# total_sample = 3628\n",
    "# num_spike    = 903\n",
    "# ((total_sample-num_spike)*2)/(num_spike*2) = 3.032\n",
    "LOSS_POS_WEIGHT       = torch.tensor([3.032])\n",
    "\n",
    "\n",
    "MODEL_FILE_DIRC_SateLight = MODEL_FILE_DIRC + \"/SateLight\"\n",
    "MODEL_FILE_DIRC_CNN       = MODEL_FILE_DIRC + \"/CNN\"\n",
    "os.makedirs(MODEL_FILE_DIRC_CNN, exist_ok=True)\n",
    "os.makedirs(MODEL_FILE_DIRC_SateLight, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.017718715393134"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sample = 3628\n",
    "num_spike    = 903\n",
    "((total_sample-num_spike)*2)/(num_spike*2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Preparation\n",
    "* Convert the data from csv to Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data from EEG_csv/eeg1.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (3840, 19)\n",
      "Data with   spike: (3840, 19)\n",
      "Data after  split into window: (3, 1280, 19)\n",
      "Labels: (3,)\n",
      "Num spike: 3\n",
      "EEG1 has 3 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg2.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(160, 1280, 19)\n",
      "EEG2 has 160 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg3.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(153, 1280, 19)\n",
      "EEG3 has 153 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg4.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(170, 1280, 19)\n",
      "EEG4 has 170 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg5.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(148, 1280, 19)\n",
      "EEG5 has 148 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg6.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(207, 1280, 19)\n",
      "EEG6 has 207 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg7.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(147, 1280, 19)\n",
      "EEG7 has 147 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg8.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (3840, 19)\n",
      "Data with   spike: (3840, 19)\n",
      "Data after  split into window: (3, 1280, 19)\n",
      "Labels: (3,)\n",
      "Num spike: 3\n",
      "EEG8 has 3 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg9.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (3840, 19)\n",
      "Data with   spike: (3840, 19)\n",
      "Data after  split into window: (3, 1280, 19)\n",
      "Labels: (3,)\n",
      "Num spike: 3\n",
      "EEG9 has 3 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg10.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(109, 1280, 19)\n",
      "EEG10 has 109 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg11.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(251, 1280, 19)\n",
      "EEG11 has 251 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg12.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (34560, 19)\n",
      "Data with   spike: (34560, 19)\n",
      "Data after  split into window: (27, 1280, 19)\n",
      "Labels: (27,)\n",
      "Num spike: 27\n",
      "EEG12 has 27 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg13.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(1101, 1280, 19)\n",
      "EEG13 has 1101 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg14.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(190, 1280, 19)\n",
      "EEG14 has 190 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg15.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (215040, 19)\n",
      "Data with   spike: (215040, 19)\n",
      "Data after  split into window: (168, 1280, 19)\n",
      "Labels: (168,)\n",
      "Num spike: 168\n",
      "EEG15 has 168 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg16.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(89, 1280, 19)\n",
      "EEG16 has 89 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg17.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (203520, 19)\n",
      "Data with   spike: (203520, 19)\n",
      "Data after  split into window: (159, 1280, 19)\n",
      "Labels: (159,)\n",
      "Num spike: 159\n",
      "EEG17 has 159 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg18.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (533760, 19)\n",
      "Data with   spike: (533760, 19)\n",
      "Data after  split into window: (417, 1280, 19)\n",
      "Labels: (417,)\n",
      "Num spike: 417\n",
      "EEG18 has 417 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg19.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (157440, 19)\n",
      "Data with   spike: (157440, 19)\n",
      "Data after  split into window: (123, 1280, 19)\n",
      "Labels: (123,)\n",
      "Num spike: 123\n",
      "EEG19 has 123 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg20.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(96, 1280, 19)\n",
      "EEG20 has 96 windows of data \n",
      "\n",
      "\n",
      "> > > Train    data  has shape: torch.Size([2610, 19, 1280]) when duration = 10 seconds\n",
      "> > > Data after DWT has shape: torch.Size([2610, 19, 1282])\n",
      "> > > Label              shape: torch.Size([2610])\n",
      "CPU times: total: 2min 4s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eeg_num_list = list(range(1,21))\n",
    "\n",
    "dataloaders, num_data = get_dataloader(eeg_num_list, shuffle=True, get_dataDWT=False)\n",
    "    \n",
    "\n",
    "num_train_data, num_valid_data, num_test_data = num_data\n",
    "train_data, valid_data, test_data             = dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_classification_model_training(EPOCH_START, NUM_EPOCHS_CLASSIFIER, \n",
    "                                        model, MODEL_FILE_DIRC, model_name,  \n",
    "                                        df, prev_best_valid_f1,prev_best_valid_loss,\n",
    "                                        train_data, num_train_data, \n",
    "                                        valid_data, num_valid_data, \n",
    "                                        scheduler, optimizer, device, LOSS_POS_WEIGHT):\n",
    "    count = 0\n",
    "    for epoch in range(EPOCH_START, NUM_EPOCHS_CLASSIFIER):\n",
    "            ## 1. Training\n",
    "            model.train()\n",
    "            train_loss, train_metric = train_classifier(model, train_data, device, num_train_data, optimizer, LOSS_POS_WEIGHT=LOSS_POS_WEIGHT) \n",
    "            \n",
    "            ## 2. Evaluating\n",
    "            model.eval()\n",
    "            valid_loss, valid_metric = evaluate_classifier(model, valid_data, device, num_valid_data, LOSS_POS_WEIGHT=LOSS_POS_WEIGHT) \n",
    "            \n",
    "            ## 3. Show the result\n",
    "            list_data       = [train_loss, valid_loss]\n",
    "            for key in [\"precision\", \"accuracy\", \"f1_score\", \"recall\"]:\n",
    "                list_data.append(train_metric[key])\n",
    "                list_data.append(valid_metric[key])\n",
    "            df.loc[len(df)] = list_data\n",
    "            \n",
    "            print_log(f\"> > > Epoch     : {epoch}\", MODEL_FILE_DIRC)\n",
    "            print_log(f\"Train {'loss':<10}: {train_loss}\", MODEL_FILE_DIRC)\n",
    "            print_log(f\"Valid {'loss':<10}: {valid_loss}\", MODEL_FILE_DIRC)\n",
    "            for key in [\"precision\", \"accuracy\", \"f1_score\", \"recall\"]:\n",
    "                print_log(f\"Train {key:<10}: {train_metric[key]}\", MODEL_FILE_DIRC)\n",
    "                print_log(f\"Valid {key:<10}: {valid_metric[key]}\", MODEL_FILE_DIRC)\n",
    "\n",
    "            \n",
    "            ## 3.1 Plot the loss function\n",
    "            fig,ax = plt.subplots(3,2, figsize=(10,10))\n",
    "            x_data = range(len(df[\"Train Loss\"]))\n",
    "            for i, key in enumerate([\"Loss\",\"precision\", \"accuracy\", \"f1_score\", \"recall\"]):    \n",
    "                ax[i%3][i//3].plot(x_data, df[f\"Train {key}\"], label=f\"Train {key}\")\n",
    "                ax[i%3][i//3].plot(x_data, df[f\"Valid {key}\"], label=f\"Valid {key}\")\n",
    "                ax[i%3][i//3].legend()\n",
    "            plt.savefig(f'{MODEL_FILE_DIRC}/Loss.png', transparent=False, facecolor='white')\n",
    "            plt.close('all')\n",
    "\n",
    "            ## 3.3. Save model and Stoping criteria\n",
    "            if prev_best_valid_f1 <= valid_metric[\"f1_score\"]:  # If previous best validation f1-score <= current f1-score \n",
    "                state_dict = {\n",
    "                    \"model\": model.state_dict(), \n",
    "                    \"epoch\":epoch,\n",
    "                    \"valid_f1_score\": valid_metric[\"f1_score\"],\n",
    "                    \"valid_loss\": valid_loss\n",
    "                }\n",
    "                torch.save(state_dict, f\"{MODEL_FILE_DIRC}/{model_name}_best.pt\")\n",
    "                prev_best_valid_f1 = valid_metric[\"f1_score\"]  # Previous validation loss = Current validation loss\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                state_dict = {\n",
    "                    \"model\": model.state_dict(), \n",
    "                    \"epoch\":epoch,\n",
    "                    \"valid_f1_score\": valid_metric[\"f1_score\"],\n",
    "                    \"valid_loss\": valid_loss\n",
    "                }\n",
    "                torch.save(state_dict, f\"{MODEL_FILE_DIRC}/{model_name}_{epoch}.pt\")\n",
    "            \n",
    "            df.to_csv(f\"{MODEL_FILE_DIRC}/Loss.csv\", index=False)\n",
    "            \n",
    "            if count == MAX_COUNT_F1_SCORE:\n",
    "                print_log(f\"The validation f1 score is not increasing for continuous {MAX_COUNT_F1_SCORE} time, so training stop\", MODEL_FILE_DIRC)\n",
    "                break\n",
    "            \n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classification_model_dict(model, MODEL_FILE_DIRC, model_name):\n",
    "    list_model = os.listdir(MODEL_FILE_DIRC) \n",
    "    if len(list_model) > 0:    # Load the latest trained model\n",
    "        if os.path.exists(f\"{MODEL_FILE_DIRC}/{model_name}_best.pt\"):\n",
    "            state_dict_loaded    = torch.load(f\"{MODEL_FILE_DIRC}/{model_name}_best.pt\")\n",
    "            prev_best_valid_f1   = state_dict_loaded[\"valid_f1_score\"]\n",
    "            prev_best_valid_loss = state_dict_loaded[\"valid_loss\"]\n",
    "        list_model.remove(f\"{model_name}_best.pt\")\n",
    "        num_list   = [int(model_dir[model_dir.rindex(\"_\") +1: model_dir.rindex(\".\")]) for model_dir in list_model if model_dir.endswith(\".pt\")]\n",
    "        num_max    = np.max(num_list)\n",
    "        \n",
    "        state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC}/{model_name}_{num_max}.pt\")\n",
    "        model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "        EPOCH_START = state_dict_loaded[\"epoch\"] + 1\n",
    "        \n",
    "        print(f\"The model has been loaded from the file '{model_name}_{num_max}.pt'\")\n",
    "\n",
    "        if os.path.exists(f\"{MODEL_FILE_DIRC}/Loss.csv\"):\n",
    "            df = pd.read_csv(f\"{MODEL_FILE_DIRC}/Loss.csv\")\n",
    "            df = df.iloc[:EPOCH_START-1, :]\n",
    "            print(f\"The dataframe that record the loss have been loaded from {MODEL_FILE_DIRC}/Loss.csv\")\n",
    "\n",
    "    else:\n",
    "        EPOCH_START            = 1\n",
    "        prev_best_valid_f1     = -1\n",
    "        prev_best_valid_loss   = 10000\n",
    "        df                     = pd.DataFrame(columns = [\"Train Loss\", \"Valid Loss\"] + \\\n",
    "                                                         flatten_concatenation([[f\"Train {metric}\", f\"Valid {metric}\"] for metric in [\"precision\", \"accuracy\", \"f1_score\", \"recall\"]]) )\n",
    "    return model, df, EPOCH_START, prev_best_valid_f1, prev_best_valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build and Train the SateLight model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build the SateLight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Sequential: 1-1                             [-1, 32, 1, 641]          --\n",
      "|    └─Conv2d: 2-1                            [-1, 16, 19, 641]         10,256\n",
      "|    └─Conv2d: 2-2                            [-1, 32, 1, 641]          640\n",
      "├─BatchNorm1d: 1-2                            [-1, 32, 641]             64\n",
      "├─ReLU: 1-3                                   [-1, 32, 641]             --\n",
      "├─Dropout: 1-4                                [-1, 32, 641]             --\n",
      "├─MaxPool1d: 1-5                              [-1, 32, 160]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-3                        [-1, 160, 32]             --\n",
      "|    |    └─SelfAttention: 3-1                [-1, 160, 32]             7,392\n",
      "|    |    └─BatchNorm1d: 3-2                  [-1, 160, 32]             320\n",
      "|    |    └─Dropout: 3-3                      [-1, 160, 32]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-4                        [-1, 160, 64]             --\n",
      "|    |    └─Linear: 3-4                       [-1, 160, 64]             2,112\n",
      "|    |    └─BatchNorm1d: 3-5                  [-1, 160, 64]             320\n",
      "|    |    └─ReLU: 3-6                         [-1, 160, 64]             --\n",
      "|    |    └─Dropout: 3-7                      [-1, 160, 64]             --\n",
      "├─MaxPool1d: 1-6                              [-1, 64, 40]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-5                        [-1, 40, 64]              --\n",
      "|    |    └─SelfAttention: 3-8                [-1, 40, 64]              29,120\n",
      "|    |    └─BatchNorm1d: 3-9                  [-1, 40, 64]              80\n",
      "|    |    └─Dropout: 3-10                     [-1, 40, 64]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-6                        [-1, 40, 96]              --\n",
      "|    |    └─Linear: 3-11                      [-1, 40, 96]              6,240\n",
      "|    |    └─BatchNorm1d: 3-12                 [-1, 40, 96]              80\n",
      "|    |    └─ReLU: 3-13                        [-1, 40, 96]              --\n",
      "|    |    └─Dropout: 3-14                     [-1, 40, 96]              --\n",
      "├─MaxPool1d: 1-7                              [-1, 96, 10]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-7                        [-1, 10, 96]              --\n",
      "|    |    └─SelfAttention: 3-15               [-1, 10, 96]              65,184\n",
      "|    |    └─BatchNorm1d: 3-16                 [-1, 10, 96]              20\n",
      "|    |    └─Dropout: 3-17                     [-1, 10, 96]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-8                        [-1, 10, 128]             --\n",
      "|    |    └─Linear: 3-18                      [-1, 10, 128]             12,416\n",
      "|    |    └─BatchNorm1d: 3-19                 [-1, 10, 128]             20\n",
      "|    |    └─ReLU: 3-20                        [-1, 10, 128]             --\n",
      "|    |    └─Dropout: 3-21                     [-1, 10, 128]             --\n",
      "├─MaxPool1d: 1-8                              [-1, 128, 2]              --\n",
      "├─Linear: 1-9                                 [-1, 1]                   257\n",
      "===============================================================================================\n",
      "Total params: 134,521\n",
      "Trainable params: 134,521\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 125.46\n",
      "===============================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 2.90\n",
      "===============================================================================================\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Sequential: 1-1                             [-1, 32, 1, 641]          --\n",
      "|    └─Conv2d: 2-1                            [-1, 16, 19, 641]         10,256\n",
      "|    └─Conv2d: 2-2                            [-1, 32, 1, 641]          640\n",
      "├─BatchNorm1d: 1-2                            [-1, 32, 641]             64\n",
      "├─ReLU: 1-3                                   [-1, 32, 641]             --\n",
      "├─Dropout: 1-4                                [-1, 32, 641]             --\n",
      "├─MaxPool1d: 1-5                              [-1, 32, 160]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-3                        [-1, 160, 32]             --\n",
      "|    |    └─SelfAttention: 3-1                [-1, 160, 32]             7,392\n",
      "|    |    └─BatchNorm1d: 3-2                  [-1, 160, 32]             320\n",
      "|    |    └─Dropout: 3-3                      [-1, 160, 32]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-4                        [-1, 160, 64]             --\n",
      "|    |    └─Linear: 3-4                       [-1, 160, 64]             2,112\n",
      "|    |    └─BatchNorm1d: 3-5                  [-1, 160, 64]             320\n",
      "|    |    └─ReLU: 3-6                         [-1, 160, 64]             --\n",
      "|    |    └─Dropout: 3-7                      [-1, 160, 64]             --\n",
      "├─MaxPool1d: 1-6                              [-1, 64, 40]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-5                        [-1, 40, 64]              --\n",
      "|    |    └─SelfAttention: 3-8                [-1, 40, 64]              29,120\n",
      "|    |    └─BatchNorm1d: 3-9                  [-1, 40, 64]              80\n",
      "|    |    └─Dropout: 3-10                     [-1, 40, 64]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-6                        [-1, 40, 96]              --\n",
      "|    |    └─Linear: 3-11                      [-1, 40, 96]              6,240\n",
      "|    |    └─BatchNorm1d: 3-12                 [-1, 40, 96]              80\n",
      "|    |    └─ReLU: 3-13                        [-1, 40, 96]              --\n",
      "|    |    └─Dropout: 3-14                     [-1, 40, 96]              --\n",
      "├─MaxPool1d: 1-7                              [-1, 96, 10]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-7                        [-1, 10, 96]              --\n",
      "|    |    └─SelfAttention: 3-15               [-1, 10, 96]              65,184\n",
      "|    |    └─BatchNorm1d: 3-16                 [-1, 10, 96]              20\n",
      "|    |    └─Dropout: 3-17                     [-1, 10, 96]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-8                        [-1, 10, 128]             --\n",
      "|    |    └─Linear: 3-18                      [-1, 10, 128]             12,416\n",
      "|    |    └─BatchNorm1d: 3-19                 [-1, 10, 128]             20\n",
      "|    |    └─ReLU: 3-20                        [-1, 10, 128]             --\n",
      "|    |    └─Dropout: 3-21                     [-1, 10, 128]             --\n",
      "├─MaxPool1d: 1-8                              [-1, 128, 2]              --\n",
      "├─Linear: 1-9                                 [-1, 1]                   257\n",
      "===============================================================================================\n",
      "Total params: 134,521\n",
      "Trainable params: 134,521\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 125.46\n",
      "===============================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 2.90\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model      = SateLight().to(device)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Get the following information:\n",
    "# 1. Previous Trained model (if exist)\n",
    "# 2. df that store the training/validation loss & metrics\n",
    "# 3. epoch where the training start\n",
    "# 4. Previous Highest Validation F1-score\n",
    "# 5. Previous Lowest  Validation Loss\n",
    "model, df, EPOCH_START, prev_best_valid_f1, prev_best_valid_loss = load_classification_model_dict(model, MODEL_FILE_DIRC_SateLight, \"SateLight\")\n",
    "\n",
    "# Get the summary of the model\n",
    "print(summary(model, (19,1280)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Print out the model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model infomation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Device used        : cuda\n",
      "BATCH SIZE         : 32\n",
      "MAX_COUNT_F1_SCORE : 10\n",
      "LEARNING RATE      : 0.001\n",
      "Prev Best f1-score in validation dataset: -1\n",
      "Prev Best validation loss               : 10000\n",
      "Number of EPOCH for training   : 101 (EPOCH start from 1)\n",
      "Num of epochs of data for train: 2542\n",
      "Num of epochs of data for valid: 543\n",
      "Model parameters               : 134,521\n"
     ]
    }
   ],
   "source": [
    "seperate = \"\\n\" + \"-\" * 100 + \"\\n\"\n",
    "print(seperate + \"Model infomation\" + seperate)\n",
    "print(f\"Device used        :\", device)\n",
    "print(f\"BATCH SIZE         :\", BATCH_SIZE)\n",
    "print(f\"MAX_COUNT_F1_SCORE :\", MAX_COUNT_F1_SCORE)\n",
    "print(f\"LEARNING RATE      :\", LEARNING_RATE)\n",
    "print(f\"Prev Best f1-score in validation dataset:\", prev_best_valid_f1)\n",
    "print(f\"Prev Best validation loss               :\", prev_best_valid_loss)\n",
    "print(f\"Number of EPOCH for training   :\",NUM_EPOCHS_CLASSIFIER, f\"(EPOCH start from {EPOCH_START})\")\n",
    "print(f\"Num of epochs of data for train:\", num_train_data)\n",
    "print(f\"Num of epochs of data for valid:\", num_valid_data)\n",
    "print(f'Model parameters               : {sum(p.numel() for p in model.parameters()):,}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > Epoch     : 1\n",
      "Train loss      : 0.016024461286328402\n",
      "Valid loss      : 0.012869781325699875\n",
      "Train precision : 0.7213541666666666\n",
      "Valid precision : 0.8037974683544303\n",
      "Train accuracy  : 0.8839496459480723\n",
      "Valid accuracy  : 0.9300184162062615\n",
      "Train f1_score  : 0.789736279401283\n",
      "Valid f1_score  : 0.8698630136986302\n",
      "Train recall    : 0.8724409448818897\n",
      "Valid recall    : 0.9477611940298507\n",
      "> > > Epoch     : 2\n",
      "Train loss      : 0.003605944743713949\n",
      "Valid loss      : 0.0036866476130714677\n",
      "Train precision : 0.9523809523809523\n",
      "Valid precision : 0.9705882352941176\n",
      "Train accuracy  : 0.981904012588513\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 0.9642301710730948\n",
      "Valid f1_score  : 0.9777777777777777\n",
      "Train recall    : 0.9763779527559056\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 3\n",
      "Train loss      : 0.0009724713005308481\n",
      "Valid loss      : 0.00729109709383491\n",
      "Train precision : 0.9859375\n",
      "Valid precision : 0.9767441860465116\n",
      "Train accuracy  : 0.9948859166011015\n",
      "Valid accuracy  : 0.9797421731123389\n",
      "Train f1_score  : 0.9898039215686274\n",
      "Valid f1_score  : 0.9581749049429658\n",
      "Train recall    : 0.9937007874015747\n",
      "Valid recall    : 0.9402985074626866\n",
      "> > > Epoch     : 4\n",
      "Train loss      : 0.00036154250315050207\n",
      "Valid loss      : 0.0042405899607235115\n",
      "Train precision : 0.9937304075235109\n",
      "Valid precision : 0.9777777777777777\n",
      "Train accuracy  : 0.9980330448465775\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 0.9960722702278083\n",
      "Valid f1_score  : 0.9814126394052045\n",
      "Train recall    : 0.9984251968503937\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 5\n",
      "Train loss      : 0.00022738628231170369\n",
      "Valid loss      : 0.011413327940957514\n",
      "Train precision : 0.9937304075235109\n",
      "Valid precision : 0.9836065573770492\n",
      "Train accuracy  : 0.9980330448465775\n",
      "Valid accuracy  : 0.9705340699815838\n",
      "Train f1_score  : 0.9960722702278083\n",
      "Valid f1_score  : 0.9375\n",
      "Train recall    : 0.9984251968503937\n",
      "Valid recall    : 0.8955223880597015\n",
      "> > > Epoch     : 6\n",
      "Train loss      : 0.0003958692653105326\n",
      "Valid loss      : 0.009934329152029306\n",
      "Train precision : 0.9968454258675079\n",
      "Valid precision : 0.9562043795620438\n",
      "Train accuracy  : 0.9980330448465775\n",
      "Valid accuracy  : 0.9834254143646409\n",
      "Train f1_score  : 0.996059889676911\n",
      "Valid f1_score  : 0.966789667896679\n",
      "Train recall    : 0.9952755905511811\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 7\n",
      "Train loss      : 0.00029030461818045937\n",
      "Valid loss      : 0.006410821912060421\n",
      "Train precision : 0.9937304075235109\n",
      "Valid precision : 0.9774436090225563\n",
      "Train accuracy  : 0.9980330448465775\n",
      "Valid accuracy  : 0.9871086556169429\n",
      "Train f1_score  : 0.9960722702278083\n",
      "Valid f1_score  : 0.9737827715355806\n",
      "Train recall    : 0.9984251968503937\n",
      "Valid recall    : 0.9701492537313433\n",
      "> > > Epoch     : 8\n",
      "Train loss      : 1.2745283188936277e-06\n",
      "Valid loss      : 0.007991784521369786\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9432624113475178\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9834254143646409\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9672727272727273\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 9\n",
      "Train loss      : 1.9661844684302833e-06\n",
      "Valid loss      : 0.011128108158645874\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.8926174496644296\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9686924493554327\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9399293286219081\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 10\n",
      "Train loss      : 1.308746763895065e-05\n",
      "Valid loss      : 0.008885235586493265\n",
      "Train precision : 0.9984276729559748\n",
      "Valid precision : 0.9361702127659575\n",
      "Train accuracy  : 0.9996066089693155\n",
      "Valid accuracy  : 0.9797421731123389\n",
      "Train f1_score  : 0.999213217938631\n",
      "Valid f1_score  : 0.96\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 11\n",
      "Train loss      : 0.0003387834284463243\n",
      "Valid loss      : 0.0077148289522351214\n",
      "Train precision : 0.9984276729559748\n",
      "Valid precision : 0.9428571428571428\n",
      "Train accuracy  : 0.9996066089693155\n",
      "Valid accuracy  : 0.9815837937384899\n",
      "Train f1_score  : 0.999213217938631\n",
      "Valid f1_score  : 0.9635036496350365\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 12\n",
      "Train loss      : 2.1342259975309997e-05\n",
      "Valid loss      : 0.006532341272156387\n",
      "Train precision : 0.9984276729559748\n",
      "Valid precision : 0.9770992366412213\n",
      "Train accuracy  : 0.9996066089693155\n",
      "Valid accuracy  : 0.9834254143646409\n",
      "Train f1_score  : 0.999213217938631\n",
      "Valid f1_score  : 0.9660377358490566\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9552238805970149\n",
      "> > > Epoch     : 13\n",
      "Train loss      : 3.1020647739913516e-06\n",
      "Valid loss      : 0.006387461541988689\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9772727272727273\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9852670349907919\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9699248120300752\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9626865671641791\n",
      "> > > Epoch     : 14\n",
      "Train loss      : 3.43561507515185e-07\n",
      "Valid loss      : 0.0035154188541198297\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9708029197080292\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.981549815498155\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 15\n",
      "Train loss      : 5.202136853383795e-07\n",
      "Valid loss      : 0.004036713196521982\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9776119402985075\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9776119402985075\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 16\n",
      "Train loss      : 6.814548698045273e-07\n",
      "Valid loss      : 0.004796054299212389\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9776119402985075\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9776119402985075\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 17\n",
      "Train loss      : 4.836241890911203e-07\n",
      "Valid loss      : 0.004780130997152867\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9776119402985075\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9776119402985075\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 18\n",
      "Train loss      : 9.378222661286955e-08\n",
      "Valid loss      : 0.0047296479170414974\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9776119402985075\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9776119402985075\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 19\n",
      "Train loss      : 1.1305660005975003e-07\n",
      "Valid loss      : 0.005025487696231996\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9776119402985075\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9776119402985075\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 20\n",
      "Train loss      : 0.0001507354120040698\n",
      "Valid loss      : 0.005161151779135809\n",
      "Train precision : 0.9968602825745683\n",
      "Valid precision : 0.9774436090225563\n",
      "Train accuracy  : 0.999213217938631\n",
      "Valid accuracy  : 0.9871086556169429\n",
      "Train f1_score  : 0.9984276729559748\n",
      "Valid f1_score  : 0.9737827715355806\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9701492537313433\n",
      "> > > Epoch     : 21\n",
      "Train loss      : 1.4456695638391983e-06\n",
      "Valid loss      : 0.004364302705911186\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9568345323741008\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9871086556169429\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9743589743589743\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 22\n",
      "Train loss      : 1.6633468669528407e-07\n",
      "Valid loss      : 0.0039025057795005896\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9708029197080292\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.981549815498155\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 23\n",
      "Train loss      : 1.7523459838532288e-07\n",
      "Valid loss      : 0.0036418955663084724\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9708029197080292\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.981549815498155\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 24\n",
      "Train loss      : 6.907920825517204e-08\n",
      "Valid loss      : 0.0037479697817117265\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9708029197080292\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.981549815498155\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 25\n",
      "Train loss      : 3.686378392389753e-06\n",
      "Valid loss      : 0.004461821478130508\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9705882352941176\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9777777777777777\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 26\n",
      "Train loss      : 4.1130603369940014e-07\n",
      "Valid loss      : 0.005306088926301148\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9637681159420289\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9779411764705882\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 27\n",
      "Train loss      : 5.357065035544048e-08\n",
      "Valid loss      : 0.005021757483190451\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.95\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9852670349907919\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9708029197080292\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 28\n",
      "Train loss      : 1.2197382635622072e-07\n",
      "Valid loss      : 0.005963579370816235\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9492753623188406\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9815837937384899\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9632352941176471\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 29\n",
      "Train loss      : 2.8895428497279906e-07\n",
      "Valid loss      : 0.005596287865018542\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.95\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9852670349907919\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9708029197080292\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 30\n",
      "Train loss      : 2.0847384200511502e-07\n",
      "Valid loss      : 0.005628022001393767\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9705882352941176\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9777777777777777\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 31\n",
      "Train loss      : 6.75441626441364e-08\n",
      "Valid loss      : 0.005823115546643687\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9637681159420289\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9779411764705882\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 32\n",
      "Train loss      : 3.0469285262867974e-08\n",
      "Valid loss      : 0.005615025502762401\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9637681159420289\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9779411764705882\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 33\n",
      "Train loss      : 2.3469775453624272e-08\n",
      "Valid loss      : 0.006062092710855042\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9432624113475178\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9834254143646409\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9672727272727273\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 34\n",
      "Train loss      : 7.08161418216387e-08\n",
      "Valid loss      : 0.0050725885034447905\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9705882352941176\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9777777777777777\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "The validation f1 score is not increasing for continuous 10 time, so training stop\n",
      "CPU times: total: 4min 47s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_classification_model_training(EPOCH_START, NUM_EPOCHS_CLASSIFIER, \n",
    "                                    model, MODEL_FILE_DIRC_SateLight, \"SateLight\",\n",
    "                                    df, prev_best_valid_f1, prev_best_valid_loss, \n",
    "                                    train_data, num_train_data, \n",
    "                                    valid_data, num_valid_data, \n",
    "                                    scheduler, optimizer, device, LOSS_POS_WEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the model performance on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is at epoch: 24\n",
      "Metric on testing dataset:\n",
      "precision : 0.7043\n",
      "accuracy  : 0.8932\n",
      "f1_score  : 0.8187\n",
      "recall    : 0.9776\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and turn to evaluation mode\n",
    "model.eval()\n",
    "state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC_SateLight}/SateLight_best.pt\")\n",
    "model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "\n",
    "test_loss, test_metric = evaluate_classifier(model, test_data, device, num_test_data) \n",
    "\n",
    "print(\"Best model is at epoch:\",state_dict_loaded[\"epoch\"])\n",
    "print(\"Metric on testing dataset:\")\n",
    "for key, value in test_metric.items():\n",
    "    print(f\"{key:<10}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Simple CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been loaded from the file 'CNN_20.pt'\n",
      "The dataframe that record the loss have been loaded from Model/CNN/Loss.csv\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Sequential: 1-1                             [-1, 128, 8]              --\n",
      "|    └─Conv1dWithInitialization: 2-1          [-1, 128, 1278]           --\n",
      "|    |    └─Conv1d: 3-1                       [-1, 128, 1278]           7,424\n",
      "|    └─BatchNorm1d: 2-2                       [-1, 128, 1278]           256\n",
      "|    └─ReLU: 2-3                              [-1, 128, 1278]           --\n",
      "|    └─MaxPool1d: 2-4                         [-1, 128, 639]            --\n",
      "|    └─Conv1dWithInitialization: 2-5          [-1, 64, 637]             --\n",
      "|    |    └─Conv1d: 3-2                       [-1, 64, 637]             24,640\n",
      "|    └─BatchNorm1d: 2-6                       [-1, 64, 637]             128\n",
      "|    └─ReLU: 2-7                              [-1, 64, 637]             --\n",
      "|    └─MaxPool1d: 2-8                         [-1, 64, 318]             --\n",
      "|    └─Conv1dWithInitialization: 2-9          [-1, 32, 316]             --\n",
      "|    |    └─Conv1d: 3-3                       [-1, 32, 316]             6,176\n",
      "|    └─BatchNorm1d: 2-10                      [-1, 32, 316]             64\n",
      "|    └─ReLU: 2-11                             [-1, 32, 316]             --\n",
      "|    └─MaxPool1d: 2-12                        [-1, 32, 158]             --\n",
      "|    └─Conv1dWithInitialization: 2-13         [-1, 16, 156]             --\n",
      "|    |    └─Conv1d: 3-4                       [-1, 16, 156]             1,552\n",
      "|    └─BatchNorm1d: 2-14                      [-1, 16, 156]             32\n",
      "|    └─ReLU: 2-15                             [-1, 16, 156]             --\n",
      "|    └─MaxPool1d: 2-16                        [-1, 16, 78]              --\n",
      "|    └─Conv1dWithInitialization: 2-17         [-1, 32, 76]              --\n",
      "|    |    └─Conv1d: 3-5                       [-1, 32, 76]              1,568\n",
      "|    └─BatchNorm1d: 2-18                      [-1, 32, 76]              64\n",
      "|    └─ReLU: 2-19                             [-1, 32, 76]              --\n",
      "|    └─MaxPool1d: 2-20                        [-1, 32, 38]              --\n",
      "|    └─Conv1dWithInitialization: 2-21         [-1, 64, 36]              --\n",
      "|    |    └─Conv1d: 3-6                       [-1, 64, 36]              6,208\n",
      "|    └─BatchNorm1d: 2-22                      [-1, 64, 36]              128\n",
      "|    └─ReLU: 2-23                             [-1, 64, 36]              --\n",
      "|    └─MaxPool1d: 2-24                        [-1, 64, 18]              --\n",
      "|    └─Conv1dWithInitialization: 2-25         [-1, 128, 16]             --\n",
      "|    |    └─Conv1d: 3-7                       [-1, 128, 16]             24,704\n",
      "|    └─BatchNorm1d: 2-26                      [-1, 128, 16]             256\n",
      "|    └─ReLU: 2-27                             [-1, 128, 16]             --\n",
      "|    └─MaxPool1d: 2-28                        [-1, 128, 8]              --\n",
      "├─Linear: 1-2                                 [-1, 1]                   1,025\n",
      "===============================================================================================\n",
      "Total params: 74,225\n",
      "Trainable params: 74,225\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 28.04\n",
      "===============================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 3.41\n",
      "Params size (MB): 0.28\n",
      "Estimated Total Size (MB): 3.79\n",
      "===============================================================================================\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Sequential: 1-1                             [-1, 128, 8]              --\n",
      "|    └─Conv1dWithInitialization: 2-1          [-1, 128, 1278]           --\n",
      "|    |    └─Conv1d: 3-1                       [-1, 128, 1278]           7,424\n",
      "|    └─BatchNorm1d: 2-2                       [-1, 128, 1278]           256\n",
      "|    └─ReLU: 2-3                              [-1, 128, 1278]           --\n",
      "|    └─MaxPool1d: 2-4                         [-1, 128, 639]            --\n",
      "|    └─Conv1dWithInitialization: 2-5          [-1, 64, 637]             --\n",
      "|    |    └─Conv1d: 3-2                       [-1, 64, 637]             24,640\n",
      "|    └─BatchNorm1d: 2-6                       [-1, 64, 637]             128\n",
      "|    └─ReLU: 2-7                              [-1, 64, 637]             --\n",
      "|    └─MaxPool1d: 2-8                         [-1, 64, 318]             --\n",
      "|    └─Conv1dWithInitialization: 2-9          [-1, 32, 316]             --\n",
      "|    |    └─Conv1d: 3-3                       [-1, 32, 316]             6,176\n",
      "|    └─BatchNorm1d: 2-10                      [-1, 32, 316]             64\n",
      "|    └─ReLU: 2-11                             [-1, 32, 316]             --\n",
      "|    └─MaxPool1d: 2-12                        [-1, 32, 158]             --\n",
      "|    └─Conv1dWithInitialization: 2-13         [-1, 16, 156]             --\n",
      "|    |    └─Conv1d: 3-4                       [-1, 16, 156]             1,552\n",
      "|    └─BatchNorm1d: 2-14                      [-1, 16, 156]             32\n",
      "|    └─ReLU: 2-15                             [-1, 16, 156]             --\n",
      "|    └─MaxPool1d: 2-16                        [-1, 16, 78]              --\n",
      "|    └─Conv1dWithInitialization: 2-17         [-1, 32, 76]              --\n",
      "|    |    └─Conv1d: 3-5                       [-1, 32, 76]              1,568\n",
      "|    └─BatchNorm1d: 2-18                      [-1, 32, 76]              64\n",
      "|    └─ReLU: 2-19                             [-1, 32, 76]              --\n",
      "|    └─MaxPool1d: 2-20                        [-1, 32, 38]              --\n",
      "|    └─Conv1dWithInitialization: 2-21         [-1, 64, 36]              --\n",
      "|    |    └─Conv1d: 3-6                       [-1, 64, 36]              6,208\n",
      "|    └─BatchNorm1d: 2-22                      [-1, 64, 36]              128\n",
      "|    └─ReLU: 2-23                             [-1, 64, 36]              --\n",
      "|    └─MaxPool1d: 2-24                        [-1, 64, 18]              --\n",
      "|    └─Conv1dWithInitialization: 2-25         [-1, 128, 16]             --\n",
      "|    |    └─Conv1d: 3-7                       [-1, 128, 16]             24,704\n",
      "|    └─BatchNorm1d: 2-26                      [-1, 128, 16]             256\n",
      "|    └─ReLU: 2-27                             [-1, 128, 16]             --\n",
      "|    └─MaxPool1d: 2-28                        [-1, 128, 8]              --\n",
      "├─Linear: 1-2                                 [-1, 1]                   1,025\n",
      "===============================================================================================\n",
      "Total params: 74,225\n",
      "Trainable params: 74,225\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 28.04\n",
      "===============================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 3.41\n",
      "Params size (MB): 0.28\n",
      "Estimated Total Size (MB): 3.79\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model      = CNN().to(device)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "scheduler  = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Get the following information:\n",
    "# 1. Previous Trained model (if exist)\n",
    "# 2. df that store the training/validation loss & metrics\n",
    "# 3. epoch where the training start\n",
    "# 4. Previous Highest Validation F1-score\n",
    "# 5. Previous Lowest  Validation Loss\n",
    "model, df, EPOCH_START, prev_best_valid_f1, prev_best_valid_loss = load_classification_model_dict(model, MODEL_FILE_DIRC_CNN, \"CNN\")\n",
    "\n",
    "# Get the summary of the model\n",
    "print(summary(model, (19,1280)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model infomation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Device used        : cuda\n",
      "BATCH SIZE         : 32\n",
      "MAX_COUNT_F1_SCORE : 10\n",
      "LEARNING RATE      : 0.001\n",
      "Prev Best recall in validation dataset: -1\n",
      "Prev Best validation loss             : 10000\n",
      "Number of EPOCH for training   : 101 (EPOCH start from 1)\n",
      "Num of epochs of data for train: 2542\n",
      "Num of epochs of data for valid: 543\n",
      "Model parameters               : 74,225\n"
     ]
    }
   ],
   "source": [
    "seperate = \"\\n\" + \"-\" * 100 + \"\\n\"\n",
    "print(seperate + \"Model infomation\" + seperate)\n",
    "print(f\"Device used        :\", device)\n",
    "print(f\"BATCH SIZE         :\", BATCH_SIZE)\n",
    "print(f\"MAX_COUNT_F1_SCORE :\", MAX_COUNT_F1_SCORE)\n",
    "print(f\"LEARNING RATE      :\", LEARNING_RATE)\n",
    "print(f\"Prev Best recall in validation dataset:\", prev_best_valid_f1)\n",
    "print(f\"Prev Best validation loss             :\", prev_best_valid_loss)\n",
    "print(f\"Number of EPOCH for training   :\",NUM_EPOCHS_CLASSIFIER, f\"(EPOCH start from {EPOCH_START})\")\n",
    "print(f\"Num of epochs of data for train:\", num_train_data)\n",
    "print(f\"Num of epochs of data for valid:\", num_valid_data)\n",
    "print(f'Model parameters               : {sum(p.numel() for p in model.parameters()):,}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > Epoch     : 1\n",
      "Train loss      : 0.007718846433994563\n",
      "Valid loss      : 0.0042417199573004495\n",
      "Train precision : 0.8393113342898135\n",
      "Valid precision : 0.900709219858156\n",
      "Train accuracy  : 0.9362706530291109\n",
      "Valid accuracy  : 0.9613259668508287\n",
      "Train f1_score  : 0.8783783783783784\n",
      "Valid f1_score  : 0.9236363636363636\n",
      "Train recall    : 0.9212598425196851\n",
      "Valid recall    : 0.9477611940298507\n",
      "> > > Epoch     : 2\n",
      "Train loss      : 0.0007576334199437287\n",
      "Valid loss      : 0.0013085228423579402\n",
      "Train precision : 0.9829192546583851\n",
      "Valid precision : 0.9568345323741008\n",
      "Train accuracy  : 0.9948859166011015\n",
      "Valid accuracy  : 0.9871086556169429\n",
      "Train f1_score  : 0.9898358092259578\n",
      "Valid f1_score  : 0.9743589743589743\n",
      "Train recall    : 0.9968503937007874\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 3\n",
      "Train loss      : 0.0006958726925133132\n",
      "Valid loss      : 0.0047371883114160325\n",
      "Train precision : 0.9936808846761453\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.996066089693155\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 0.9921135646687698\n",
      "Valid f1_score  : 0.9770992366412213\n",
      "Train recall    : 0.9905511811023622\n",
      "Valid recall    : 0.9552238805970149\n",
      "> > > Epoch     : 4\n",
      "Train loss      : 0.0007630319546435924\n",
      "Valid loss      : 0.006534392827686349\n",
      "Train precision : 0.9813953488372092\n",
      "Valid precision : 0.9154929577464789\n",
      "Train accuracy  : 0.994492525570417\n",
      "Valid accuracy  : 0.9705340699815838\n",
      "Train f1_score  : 0.9890625\n",
      "Valid f1_score  : 0.9420289855072463\n",
      "Train recall    : 0.9968503937007874\n",
      "Valid recall    : 0.9701492537313433\n",
      "> > > Epoch     : 5\n",
      "Train loss      : 0.00027069712287460505\n",
      "Valid loss      : 0.01629570394181552\n",
      "Train precision : 0.9937402190923318\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.998426435877262\n",
      "Valid accuracy  : 0.9742173112338858\n",
      "Train f1_score  : 0.9968602825745683\n",
      "Valid f1_score  : 0.9448818897637795\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.8955223880597015\n",
      "> > > Epoch     : 6\n",
      "Train loss      : 0.0003108032383584672\n",
      "Valid loss      : 0.0025790402844269944\n",
      "Train precision : 0.9937205651491365\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.997639653815893\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 0.9952830188679245\n",
      "Valid f1_score  : 0.9924812030075187\n",
      "Train recall    : 0.9968503937007874\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 7\n",
      "Train loss      : 0.00011542042146303381\n",
      "Valid loss      : 0.008530590457002179\n",
      "Train precision : 0.9968553459119497\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.9988198269079465\n",
      "Valid accuracy  : 0.9871086556169429\n",
      "Train f1_score  : 0.997639653815893\n",
      "Valid f1_score  : 0.9731800766283525\n",
      "Train recall    : 0.9984251968503937\n",
      "Valid recall    : 0.9477611940298507\n",
      "> > > Epoch     : 8\n",
      "Train loss      : 0.00028334048357752533\n",
      "Valid loss      : 0.0021151849421193274\n",
      "Train precision : 0.9921752738654147\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.997639653815893\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 0.9952904238618524\n",
      "Valid f1_score  : 0.9809885931558935\n",
      "Train recall    : 0.9984251968503937\n",
      "Valid recall    : 0.9626865671641791\n",
      "> > > Epoch     : 9\n",
      "Train loss      : 0.0005631691473327281\n",
      "Valid loss      : 0.021528783336652247\n",
      "Train precision : 0.9860031104199067\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.996066089693155\n",
      "Valid accuracy  : 0.9521178637200737\n",
      "Train f1_score  : 0.9921752738654147\n",
      "Valid f1_score  : 0.8925619834710744\n",
      "Train recall    : 0.9984251968503937\n",
      "Valid recall    : 0.8059701492537313\n",
      "> > > Epoch     : 10\n",
      "Train loss      : 9.865666609915473e-05\n",
      "Valid loss      : 0.0003541657843384343\n",
      "Train precision : 0.9984276729559748\n",
      "Valid precision : 0.9852941176470589\n",
      "Train accuracy  : 0.9996066089693155\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 0.999213217938631\n",
      "Valid f1_score  : 0.9925925925925926\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 1.0\n",
      "> > > Epoch     : 11\n",
      "Train loss      : 0.0003885225844332826\n",
      "Valid loss      : 0.0006086212671403671\n",
      "Train precision : 0.9952978056426333\n",
      "Valid precision : 0.9571428571428572\n",
      "Train accuracy  : 0.9988198269079465\n",
      "Valid accuracy  : 0.988950276243094\n",
      "Train f1_score  : 0.997643362136685\n",
      "Valid f1_score  : 0.9781021897810219\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 1.0\n",
      "> > > Epoch     : 12\n",
      "Train loss      : 0.00011083319289400391\n",
      "Valid loss      : 0.0010529834186987035\n",
      "Train precision : 1.0\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.9996066089693155\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 0.9992119779353822\n",
      "Valid f1_score  : 0.9924812030075187\n",
      "Train recall    : 0.9984251968503937\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 13\n",
      "Train loss      : 7.0027560595737065e-06\n",
      "Valid loss      : 0.00033125869900836575\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9852941176470589\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9925925925925926\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 1.0\n",
      "> > > Epoch     : 14\n",
      "Train loss      : 1.3461085208862864e-05\n",
      "Valid loss      : 0.0013734228420079343\n",
      "Train precision : 1.0\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9924812030075187\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 15\n",
      "Train loss      : 9.012229635462152e-05\n",
      "Valid loss      : 0.00030356892350432416\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9925373134328358\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9925373134328358\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9925373134328358\n",
      "> > > Epoch     : 16\n",
      "Train loss      : 3.096115337321555e-05\n",
      "Valid loss      : 0.0022952945000121886\n",
      "Train precision : 0.9984276729559748\n",
      "Valid precision : 0.9850746268656716\n",
      "Train accuracy  : 0.9996066089693155\n",
      "Valid accuracy  : 0.992633517495396\n",
      "Train f1_score  : 0.999213217938631\n",
      "Valid f1_score  : 0.9850746268656716\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 17\n",
      "Train loss      : 6.399006119042649e-06\n",
      "Valid loss      : 0.003022074904706466\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.9849624060150376\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9812734082397003\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 18\n",
      "Train loss      : 2.2761317441756944e-05\n",
      "Valid loss      : 0.005856703614018957\n",
      "Train precision : 0.9984276729559748\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 0.9996066089693155\n",
      "Valid accuracy  : 0.990791896869245\n",
      "Train f1_score  : 0.999213217938631\n",
      "Valid f1_score  : 0.9809885931558935\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9626865671641791\n",
      "> > > Epoch     : 19\n",
      "Train loss      : 6.422178036967441e-06\n",
      "Valid loss      : 0.0017277857868199088\n",
      "Train precision : 1.0\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.994475138121547\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9886792452830189\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 20\n",
      "Train loss      : 1.8961743292866896e-05\n",
      "Valid loss      : 0.0025981151939409187\n",
      "Train precision : 1.0\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.994475138121547\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9886792452830189\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9776119402985075\n",
      "> > > Epoch     : 21\n",
      "Train loss      : 2.44720067680571e-05\n",
      "Valid loss      : 0.0017437971509687814\n",
      "Train precision : 1.0\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9924812030075187\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 22\n",
      "Train loss      : 1.1172987395556064e-05\n",
      "Valid loss      : 0.0018760138708734777\n",
      "Train precision : 1.0\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9924812030075187\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "> > > Epoch     : 23\n",
      "Train loss      : 1.534239618776951e-06\n",
      "Valid loss      : 0.0019071303198301046\n",
      "Train precision : 1.0\n",
      "Valid precision : 1.0\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.996316758747698\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.9924812030075187\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.9850746268656716\n",
      "The validation f1 score is not increasing for continuous 10 time, so training stop\n",
      "CPU times: total: 58.1 s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_classification_model_training(EPOCH_START, NUM_EPOCHS_CLASSIFIER, \n",
    "                                    model, MODEL_FILE_DIRC_CNN, \"CNN\",\n",
    "                                    df, prev_best_valid_f1, prev_best_valid_loss, \n",
    "                                    train_data, num_train_data, \n",
    "                                    valid_data, num_valid_data, \n",
    "                                    scheduler, optimizer, device, LOSS_POS_WEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model performance on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is at epoch: 13\n",
      "Metric on testing dataset:\n",
      "precision : 0.9416\n",
      "accuracy  : 0.9761\n",
      "f1_score  : 0.9520\n",
      "recall    : 0.9627\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and turn to evaluation mode\n",
    "model.eval()\n",
    "state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC_CNN}/CNN_best.pt\")\n",
    "model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "\n",
    "test_loss, test_metric = evaluate_classifier(model, test_data, device, num_test_data) \n",
    "\n",
    "print(\"Best model is at epoch:\",state_dict_loaded[\"epoch\"])\n",
    "print(\"Metric on testing dataset:\")\n",
    "for key, value in test_metric.items():\n",
    "    print(f\"{key:<10}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
